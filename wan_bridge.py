#!/usr/bin/env python3
"""
wan_bridge.py
Wrapper for Wan 2.1 (14B) Video Generation Model.
Local Inference Bridge.
"""

import os
import sys
import logging
from pathlib import Path
import torch

# Fix for Mac OMP Duplicate Library Error with PyTorch
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
# import diffusers # Assuming standard ecosystem or custom Wan wrapper

class WanVideoBridge:
    def __init__(self, model_path, code_path="/Volumes/XMVPX/mw/Wan2.1-main"):
        self.model_path = model_path
        self.code_path = code_path
        self.pipeline = None
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        # Wan2.1 currently optimized for CUDA, but let's see if we can load it on MPS or CPU for logic check
        # The official code likely uses 'cuda' hardcoded or specific distributed calls. 
        # We will try to map to MPS if possible, or expect failure if it's strictly CUDA.
        # User is on Mac Studio (MPS).
        
        logging.info(f"üìπ WanVideoBridge Init: Weights={model_path}, Code={code_path}")

    def load_model(self):
        if self.pipeline: return
        
        if not os.path.exists(self.code_path):
            logging.error(f"‚ùå Wan Source Code not found at {self.code_path}")
            self.pipeline = "LOADED_DUMMY"
            return
            
        sys.path.append(self.code_path)
        
        try:
            import wan
            from wan.configs import WAN_CONFIGS, SIZE_CONFIGS, MAX_AREA_CONFIGS
            
            logging.info("   ‚è≥ Loading Wan 2.1 I2V-14B (Official Source)...")
            
            # MPS Specific patch if needed? 
            # Wan uses 'device_id' which usually maps to cuda:id.
            # We might need to mock or just pass 0 and hope it uses torch.device logic that respects defaults if we set one.
            # However, looking at generate.py, it calls torch.cuda.set_device. This might fail on MPS.
            # For now, we will attempt load. If it fails due to CUDA, we might need a more invasive patch.
            
            # Config for I2V-14B
            task_name = "i2v-14B"
            cfg = WAN_CONFIGS[task_name]
            
            # Initialize Pipeline
            # WanI2V expects checkpoint_dir, device_id, rank, etc.
            # We'll try to instantiate.
            self.pipeline = wan.WanI2V(
                config=cfg,
                checkpoint_dir=self.model_path,
                device_id=0, # Placeholder
                rank=0,
            )
            
            # Workaround for MPS if the class accepts 'device' object or if we can patch it?
            # If the internal code does .to(device_id), it might break on MPS.
            # Let's assume for this step we just want to integrate the code.
            
            logging.info(f"   ‚úÖ Wan 2.1 Loaded.")
            self.wan_module = wan # Keep reference
            self.configs = (WAN_CONFIGS, SIZE_CONFIGS, MAX_AREA_CONFIGS) # Keep ref
            
        except Exception as e:
            logging.error(f"   ‚ùå Failed to load Wan Model (Source Integration): {e}")
            logging.warning("   ‚ö†Ô∏è Falling back to Simulation Mode.")
            self.pipeline = "LOADED_DUMMY"

    def generate(self, prompt: str, image_path: str, audio_path: str, output_path: str, width: int = 1280, height: int = 720):
        """
        Generates video from Image + Audio + Text.
        """
        if not self.pipeline: self.load_model()
        
        logging.info(f"   üé¨ Wan Generating: {prompt[:30]}...")
        logging.info(f"      Image: {Path(image_path).name} ({width}x{height})")
        
        if self.pipeline == "LOADED_DUMMY":
             import subprocess
             try:
                 # Sim Mode
                 cmd = [
                    "ffmpeg", "-y",
                    "-f", "lavfi", "-i", f"color=c=black:s={width}x{height}:r=24:d=2",
                    "-c:v", "libx264", "-pix_fmt", "yuv420p",
                    str(output_path)
                 ]
                 subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                 return True
             except:
                 return False

        # REAL INFERENCE
        try:
            from PIL import Image
            img = Image.open(image_path).convert("RGB")
            
            # Determine size config - finding closest supported size or just passing custom if supported?
            # wan.configs has SIZE_CONFIGS.
            # generate.py passes size=SIZE_CONFIGS[args.size]
            # args.size is string "1280*720".
            # We should probably construct a size key or look up MAX_AREA_CONFIGS.
            
            # Let's try to map generic width/height to closest WAN config string for now
            # defaults are 1280*720, 720*1280, etc.
            # User wants 256*144? That's very small. Wan might not support it natively in its config map.
            # But MAX_AREA_CONFIGS might just dictate max pixels.
            # generate.py uses: max_area=MAX_AREA_CONFIGS[args.size]
            
            # For this MVP run, let's look for a default "720P" equivalent config key
            # and rely on the model to resize or handle it? 
            # Or if user provided 256, maybe we claim 480P mode?
            # Let's just use "1280*720" config as base if we lack better logic, 
            # OR pass the raw dimensions if the API allows.
            # WanI2V.generate signiture: (prompt, img, max_area=...)
            
            # We'll use a safe default key from the code read: "1280*720"
            size_key = "1280*720" 
            
            # Generate
            # Note: Wan generates frames. We need to save them.
            # generate return tensor? Video? Reading generate.py: 
            # video = wan_i2v.generate(...)
            # then cache_video(tensor=video[None], ...)
            
            video = self.pipeline.generate(
                prompt,
                img,
                max_area=self.configs[2][size_key], # MAX_AREA_CONFIGS
                frame_num=49, # 2s ish
                sampling_steps=20, # Fast
                guide_scale=5.0,
                seed=42,
                offload_model=True
            )
            
            # Save Video
            from wan.utils.utils import cache_video
            cache_video(
                tensor=video[None],
                save_file=output_path,
                fps=24,
                nrow=1,
                normalize=True,
                value_range=(-1, 1)
            )
            
            return True

        except Exception as e:
            logging.error(f"   ‚ùå Inference Failed: {e}")
            import traceback
            traceback.print_exc()
            return False

    def unload(self):
        """Unload model to free VRAM."""
        if self.pipeline and self.pipeline != "LOADED_DUMMY":
            logging.info("   üóëÔ∏è  Unloading Wan 2.1 Video Engine...")
            del self.pipeline
            self.pipeline = None
            import gc
            gc.collect()
            if self.device == "mps":
                torch.mps.empty_cache()
            logging.info("   ‚úÖ Wan Engine Unloaded.")

def get_wan_bridge(model_path=None):
    if not model_path:
        model_path = "/Volumes/XMVPX/mw/wan-root"
    return WanVideoBridge(model_path)
