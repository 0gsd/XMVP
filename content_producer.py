#!/usr/bin/env python3
"""
content_producer.py
-------------------
Unified generator for podcast and improv content.
Merges logic from `podcast_animator.py` (Script->Video) and `improv_animator.py` (Idea->Video).

Modes:
1. Podcast/Script Animation (Triplets/Pairs)
2. Improv Comedy (Zero-shot infinite generation)

Supported Engines:
- Text: Gemini 2.0 Flash (Cloud)
- Audio: Google Journey (Cloud) OR Kokoro (Local)
- Visuals: Gemini 2.0 Flash (Cloud) OR Flux Schnell (Local)
- Foley: None OR HunyuanVideo-Foley (Local)

Author: 0i0 (Merged by Antigravity)
Date: Jan 2026
"""

import os
import sys
import json
import argparse
import glob
import time
import re
import subprocess
import random
import shutil
import logging
from pathlib import Path
import itertools

# External Libs
import requests
from PIL import Image

try:
    # Try New SDK (v1.0+)
    from google import genai
    from google.genai import types
except ImportError:
    try:
        # Try Stable SDK (v0.8.x)
        import google.generativeai as genai
        # Types shim for backward compatibility if needed, or rely on duck typing
    except ImportError:
        print("[-] Warning: No Gemini SDK found. Cloud mode will fail.")
        pass

# MVP Imports
try:
    import mvp_shared
    from mvp_shared import (
        save_xmvp, load_xmvp, CSSV, VPForm, Story, Portion, Constraints, 
        load_manifest, Manifest, Seg, DialogueScript, Indecision, DialogueLine
    )
    from text_engine import TextEngine
    from truth_safety import TruthSafety
    from foley_talk import generate_audio_asset
    from vision_producer import get_chaos_seed
    from vision_producer import get_chaos_seed
    from thax_audio import get_thax_engine
    from foley_talk import assign_kokoro_voice_deterministic
    from vision_producer import get_chaos_seed
    # frame_canvas moved to lazy import
    
    # Local Bridges
    from flux_bridge import get_flux_bridge
    from hunyuan_foley_bridge import generate_foley_asset
except ImportError as e:
    print(f"[-] Critical Import Error: {e}")
    sys.exit(1)

print("DEBUG: Imports Done.")

# --- CONFIG ---
OUTPUT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "z_test-outputs"))
TRIPLETS_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "../z_podcast_triplets"))
MW_ROOT = "/Volumes/XMVPX/mw" # Standard mount point

# Local Model Paths (Hardcoded standards for XMVP)
FLUX_MODEL_PATH = os.path.join(MW_ROOT, "flux-root")
if not os.path.exists(FLUX_MODEL_PATH):
    # Fallback to safetensors if user moved it manually, but default populate is flux-root
    FLUX_MODEL_PATH = os.path.join(MW_ROOT, "flux-schnell.safetensors")

# --- GLOBAL STATE ---
LOCAL_MODE = False
FOLEY_ENABLED = False
FLUX_BRIDGE = None

# --- CONSTANTS & FORMS ---
FORM_DEFS = {
    "24-cartoon": {
        "description": "4-Person Improv Comedy Special (24 Minutes)",
        "cast": {
            "William": {"base": "Billy Joel", "voice": "en-US-Journey-D", "pitch": 1, "persona": "The Piano Man. Working-class poet. Melodic, specific geographic references (Long Island), cynical but soulful."},
            "Maggie": {"base": "Margaret Thatcher", "voice": "en-US-Journey-F", "pitch": -2, "persona": "The Iron Lady. Stern, authoritative, uses 'Royal We'. Surprisingly willing to play high-status absurd characters."},
            "Francis": {"base": "Frank Sinatra", "voice": "en-US-Journey-L", "pitch": -2, "persona": "The Chairman. Cool, swaggering, mid-Atlantic accent. emotional, volatile, calls everyone 'baby'. Does it 'My Way'."},
            "Anne Tailored": {"base": "Taylor Swift", "voice": "en-US-Journey-O", "pitch": 1, "persona": "The Pop Icon. Earnest, confessional, detailed storytelling. Bridges scenes with emotional hooks. Avoids copyright infringement."}
        },
        "system_prompt_template": (
            "You are the Director of a long-form Improv Comedy show in a Black Box Theater.\n"
            "The Cast:\n{cast_desc}\n\n"
            "The Rules:\n"
            "1. Generate ONE turn at a time (Speaker + Dialogue + Physical Action).\n"
            "2. Maintain a coherent narrative arc weaving through the Chaos Seeds.\n"
            "3. Style: 'Yes, And', witty, character-driven.\n"
            "4. Format: JSON {{ 'speaker': 'Name', 'text': 'Dialogue', 'action_prompt': 'Visual description of action', 'visual_focus': 'Focus' }}\n"
        )
    },
    # Aliases
    "24-podcast": {"alias": "24-cartoon"},
    "10-podcast": {"alias": "24-cartoon", "duration_override": 600},
    "route66-podcast": {
        "description": "6-Person Improv Narrative (66 Minutes)",
        "duration_override": 3960,
        "cast": {
            "Amey": {"voice": "af_bella", "pitch": 0, "persona": "The Dreamer. Optimistic, sees signs everywhere, possibly psychic."},
            "Jessinny": {"voice": "af_sarah", "pitch": 0, "persona": "The Sceptic. Grounded, practical, calls out nonsense."},
            "Lorrey": {"voice": "af_nicole", "pitch": 0, "persona": "The Historian. Obsessed with the past, nostalgic, melancholic."},
            "Mercutio": {"voice": "am_michael", "pitch": 0, "persona": "The Trickster. Chaotic, plays devil's advocate, unpredictable."},
            "Rondio": {"voice": "am_adam", "pitch": 0, "persona": "The Driver. Focused, mission-oriented, protective."},
            "Totto": {"voice": "bm_george", "pitch": 0, "persona": "The Passenger. Along for the ride, observant, accidentally profound."}
        },
        "system_prompt_template": (
            "You are the Director of a 66-minute improvised audio drama featuring 6 travelers on a metaphysical road trip.\n"
            "The Cast:\n{cast_desc}\n\n"
            "The Mission:\n"
            "Weave a single coherent narrative incorporating 6 distinct Chaos Seeds over the journey.\n"
            "The Rules:\n"
            "1. Generate ONE turn at a time (Speaker + Dialogue + Physical Action).\n"
            "2. Maintain strict continuity of location and objective.\n"
            "3. Style: Atmospheric, somewhat surreal, character-driven.\n"
            "4. Format: JSON {{ 'speaker': 'Name', 'text': 'Dialogue', 'action_prompt': 'Visual description', 'visual_focus': 'Focus' }}\n"
        )
    }
}
FORM_DEFS["24-podcast"] = FORM_DEFS["24-cartoon"] # Simple alias ref

# --- RVC CONFIG ---
RVC_MODELS_ROOT = os.path.join(os.path.dirname(__file__), "z_training_data")
# Note: 24-voices are in z_training_data/24_voices, Route 66 in z_training_data/route66_voices
# We need to handle sub-roots or just full relative paths in the map.
# Let's update RVC_MAP to include "24_voices/" or "route66_voices/" prefix or handle root logic.
# Implementation Plan assumed RVC_MAP values were subdir names.
# Let's make RVC_MODELS_ROOT generic to "z_training_data" and include parent dir in map values.

RVC_MAP = {
    # 24-Podcast
    "William": "24_voices/william-content",
    "Maggie": "24_voices/maggie-content",
    "Francis": "24_voices/francis-content",
    "Anne Tailored": "24_voices/annetailored-content",
    # Route 66
    "Amey": "route66_voices/amey-content",
    "Jessinny": "route66_voices/jessinny-content",
    "Lorrey": "route66_voices/lorrey-content",
    "Mercutio": "route66_voices/mercutio-content",
    "Rondio": "route66_voices/rondio-content",
    "Totto": "route66_voices/totto-content"
}
RVC_PYTHON_BIN = os.path.join(os.path.expanduser("~"), "miniconda3/envs/rvc_env/bin/python")

COURTROOM_STYLE_PROMPT = (
    "A photorealistic courtroom sketch. Drawn by a preternaturally talented artist "
    "in an underground black box theater. Hyperrealistic style, clean commercial art. "
    "NO WORDS, NO SPEECH BUBBLES, NO TEXT. "
    "The artist captures the actors improvising, surrounded by imaginary objects. "
    "Dynamic composition, intense energy."
)

# --- HELPERS ---

def get_client_keys():
    keys = mvp_shared.load_api_keys()
    if not keys: return None
    random.shuffle(keys)
    return itertools.cycle(keys)

KEY_CYCLE = get_client_keys()

def get_client():
    if not KEY_CYCLE: return None, None
    key = next(KEY_CYCLE)
    return genai.Client(api_key=key), key

def get_audio_duration(file_path):
    try:
        cmd = ['ffprobe', '-i', file_path, '-show_entries', 'format=duration', '-v', 'quiet', '-of', 'csv=p=0']
        result = subprocess.run(cmd, capture_output=True, text=True)
        return float(result.stdout.strip())
    except:
        return 0.0

def generate_location_context(text_engine, args):
    """
    Determines the visual setting for the session.
    1. Use --location if provided.
    2. Else, generate a random, weird location.
    """
    if args.location:
        print(f"    [üåç] Location Override: {args.location}")
        return args.location
        
    print("    [üé≤] Generating Random Location...")
    prompt = (
        "Generate a specific, visually interesting, slightly weird location for a conversation to take place. "
        "Examples: 'A cyberpunk noodle shop', 'A haunted victorian hotel lobby', 'The bridge of a starship', 'A 1970s bowling alley'. "
        "Return ONLY the location description string."
    )
    try:
        loc = text_engine.generate(prompt).strip().strip('"')
        print(f"    [üåç] Location: {loc}")
        return loc
    except Exception as e:
        print(f"    [!] Location Gen Failed: {e}. using default.")
        return "An underground black box theater"

def generate_image(prompt, output_path, ts=None):
    """
    Generates an image using either:
    1. Flux Schnell (Local) if LOCAL_MODE is True
    2. Gemini 2.0 Flash (Cloud) otherwise
    """
    global FLUX_BRIDGE
    
    if LOCAL_MODE:
        # --- LOCAL FLUX ---
        if not FLUX_BRIDGE:
             print(f"   [üåä] Initializing Flux from {FLUX_MODEL_PATH}...")
             try:
                 FLUX_BRIDGE = get_flux_bridge(FLUX_MODEL_PATH)
             except Exception as e:
                 print(f"   [-] Flux Init Failed: {e}. Falling back to Black Frame.")
                 return False
        
        try:
            # Flux requires no specific aspect ratio args in prompt usually, handled by width/height
            img = FLUX_BRIDGE.generate(prompt, width=1024, height=1024, steps=4) # Schnell 4 steps
            if img:
                img.save(output_path)
                return True
            else:
                return False
        except Exception as e:
            print(f"   [-] Flux Gen Failed: {e}")
            return False
            
    else:
        # --- CLOUD GEMINI ---
        max_retries = 6 
        base_delay = 5
        
        for attempt in range(max_retries):
            client, key = get_client()
            if not client: 
                print("[-] No Keys.")
                return False
                
            try:
                final_prompt = f"{prompt} --aspect_ratio 1:1"
                if ts: final_prompt = ts.refine_prompt(prompt, pg_mode=True)
                
                response = client.models.generate_content(
                    model="gemini-2.0-flash", 
                    contents=final_prompt
                )
                if response.candidates:
                    for part in response.candidates[0].content.parts:
                        if part.inline_data:
                            with open(output_path, "wb") as f:
                                f.write(part.inline_data.data)
                            return True
            except Exception as e:
                # 429 = Resource Exhausted / Quota
                # 503 = Overloaded
                if "429" in str(e) or "503" in str(e) or "ResourceExhausted" in str(e):
                    delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                    print(f"   [‚è≥] Quota Hit (Attempt {attempt+1}/{max_retries}). Sleeping {delay:.1f}s...")
                    time.sleep(delay)
                else:
                    print(f"   [-] Cloud Gen Failed: {e}")
                    time.sleep(1) # Short pause on other errors
                    
        return False

def create_black_frame(path):
    subprocess.run(['ffmpeg', '-f', 'lavfi', '-i', 'color=c=black:s=1024x1024:d=0.1', '-frames:v', '1', path, '-y', '-loglevel', 'error'])

def map_voice_to_kokoro(cloud_voice):
    """Maps Google Cloud Journey voices to Kokoro equivalents."""
    mapping = {
        # Females
        "en-US-Journey-F": "af_bella",  # Warm, clear
        "en-US-Journey-O": "af_sarah",  # Soft, calm
        # Males
        "en-US-Journey-D": "am_michael", # Deep, resonant
        "en-US-Journey-L": "am_adam",    # Clear, neutral
        # Fallbacks
        "en-US-Journey": "af_bella"
    }
    # Naive match
    for k, v in mapping.items():
        if k in cloud_voice:
            return v
    
    # Random fallback based on letter if possible?
    # Or just default
    return "af_bella"

def run_rvc_conversion(wav_path, character_name):
    """
    Polymorphic RVC Converter.
    1. Finds model for character in z_training_data/24_voices
    2. Runs inference via rvc_env
    3. Overwrites wav_path with converted audio
    """
    if character_name not in RVC_MAP:
        print(f"       [!] RVC: No mapping for {character_name}")
        return False
        
    model_subdir = RVC_MAP[character_name]
    model_dir = os.path.join(RVC_MODELS_ROOT, model_subdir)
    
    # Strict Filename Matching (User Sanitization Request)
    # Character Name key -> lowercase filename
    # e.g. "William" -> william-content -> william.pth
    
    # Extract clean name from character_name or model_subdir?
    # model_subdir is like "24_voices/william-content"
    # We want "william" from the basename "william-content" (split -)
    # OR simpler: map allows arbitrary keys, we should just assume the filename matches the key.lower()??
    # User said: "amey.pth ... under amey-content".
    # And "William" -> "william.pth".
    # Let's derive it from the key in RVC_MAP? 
    # Key is "William", "Anne Tailored", etc.
    # Filenames I just made: "william.pth", "annetailored.pth".
    # Logic: key.lower().replace(" ", "") ?
    # "Anne Tailored" -> "annetailored". Correct.
    # "William" -> "william". Correct.
    
    clean_name = character_name.lower().replace(" ", "")
    pth_file = os.path.join(model_dir, f"{clean_name}.pth")
    index_file = os.path.join(model_dir, f"{clean_name}.index")
    
    if not os.path.exists(pth_file):
        print(f"       [!] RVC: Model file not found: {pth_file}")
        return False
        
    if not os.path.exists(index_file):
        index_file = "" # Optional
        
    # Construct Temp Path
    # We write result to a temp file first, then move it
    temp_rvc_out = wav_path.replace(".wav", "_rvc.wav")
    
    rvc_cmd = [
        RVC_PYTHON_BIN, "-c",
        f"""
import os, sys, torch
# Monkeypatch PyTorch 2.6+
_original_load = torch.load
def _unsafe_load(*args, **kwargs):
    if 'weights_only' not in kwargs: kwargs['weights_only'] = False
    return _original_load(*args, **kwargs)
torch.load = _unsafe_load

from rvc_python.infer import RVCInference
try:
    rvc = RVCInference(device="mps")
    try:
        rvc.load_model("{pth_file}", version="v2")
    except:
        rvc.load_model("{pth_file}")
        
    # Manual Single Inference
    wav_opt = rvc.vc.vc_single(
        0, "{wav_path}", 0, None, "rmvpe", 
        "{index_file if index_file else ''}", "", 0, 3, 0, 0.25, 0.33
    )
    
    if isinstance(wav_opt, tuple): tgt_sr, audio_data = wav_opt
    else: tgt_sr, audio_data = rvc.vc.tgt_sr, wav_opt

    from scipy.io import wavfile
    wavfile.write("{temp_rvc_out}", tgt_sr, audio_data)
    print("SUCCESS")
except Exception as e:
    print(f"ERROR: {{e}}")
    sys.exit(1)
"""
    ]
    
    try:
        print(f"       mic... converting to {character_name} via RVC...")
        res = subprocess.run(rvc_cmd, capture_output=True, text=True)
        if "SUCCESS" in res.stdout:
             # Success
             os.replace(temp_rvc_out, wav_path)
             # print("       [+] RVC Converison Complete") # Optional verbose
             return True
        else:
             # Capture standard out too since our script prints ERROR there sometimes
             err_msg = res.stderr[:200] if res.stderr else res.stdout[:200]
             print(f"       [!] RVC Failed: {err_msg}")
             return False
    except Exception as e:
        print(f"       [!] RVC Exec Failed: {e}")
        return False


# --- IMPROV LOGIC (From improv_animator.py) ---

def generate_dynamic_cast(text_engine, seeds):
    print("    [‚ú®] Casting Call: Generating Dynamic Personas...")
    selected_seeds = seeds[:4]
    slots = [
        {"voice": "en-US-Journey-D", "pitch": 0, "gender": "Male"},      # Deep
        {"voice": "en-US-Journey-F", "pitch": 0, "gender": "Female"},    # Warm
        {"voice": "en-US-Journey-L", "pitch": 0, "gender": "Male"},      # Neutral
        {"voice": "en-US-Journey-O", "pitch": 0, "gender": "Female"}     # Soft
    ]
    dynamic_cast = {}
    for i, seed in enumerate(selected_seeds):
        slot = slots[i]
        prompt = (f"Create a weird improv character inspired by: '{seed}'. Gender: {slot['gender']}. "
                  "JSON: { 'name': 'First Last', 'persona': 'Description' }")
        try:
            raw = text_engine.generate(prompt, json_schema=True)
            data = json.loads(raw)
            name = data.get("name", f"Player {i+1}")
            dynamic_cast[name] = {
                "base": name, "voice": slot["voice"], "pitch": slot["pitch"],
                "persona": data.get("persona", f"Improviser inspired by {seed}")
            }
            print(f"       + Cast {name}: {dynamic_cast[name]['persona'][:40]}...")
        except:
            pass
    return dynamic_cast

def generate_ensemble_cast(text_engine):
    """Generates 4-person cast (2M, 2F) using Legacy GAHD Logic."""
    print("    [‚ú®] Casting Call: Summoning Character Actors (1975-2005)...")
    
    prompt = (
        "Generate a cast of 4 distinct 'Character Actors' (2 Male, 2 Female) active 1975-2005.\n"
        "CRITERIA: Recognizable but not A-List. Distinct voices/types. Real actors or very close pastiches.\n"
        "OUTPUT JSON: "
        "[{ 'name': 'Name', 'gender': 'Male/Female', 'persona': 'Vocal/Personality description' }, ...]"
    )
    
    try:
        raw = text_engine.generate(prompt, json_schema=True)
        cast_list = json.loads(raw)
        
        # Ensure we have a list
        if isinstance(cast_list, dict) and "cast" in cast_list: cast_list = cast_list["cast"]
        
        final_cast = {}
        slots = [
            {'gender': 'Male', 'voice': 'en-US-Journey-D', 'pitch': 1},
            {'gender': 'Female', 'voice': 'en-US-Journey-F', 'pitch': -2},
            {'gender': 'Male', 'voice': 'en-US-Journey-D', 'pitch': -2},
            {'gender': 'Female', 'voice': 'en-US-Journey-F', 'pitch': 1}
        ]
        
        # Attempt to match genders to slots
        for i, actor in enumerate(cast_list[:4]):
            # Find matching slot
            slot = next((s for s in slots if s['gender'].lower() == actor.get('gender', 'Male').lower()), None)
            if not slot and slots: slot = slots[0] # Fallback
            if slot in slots: slots.remove(slot)
            
            voice = slot['voice'] if slot else 'en-US-Journey-D'
            pitch = slot['pitch'] if slot else 0
            
            final_cast[actor['name']] = {
                "base": actor['name'],
                "voice": voice, 
                "pitch": pitch,
                "persona": actor.get('persona', 'A character actor.')
            }
            print(f"       + Cast {actor['name']} as {voice} (p{pitch})")
            
        return final_cast
        
    except Exception as e:
        print(f"    [-] Casting Failed: {e}. Using Default Backup.")
        return {
            "Joe Pantoliano": {"voice": "en-US-Journey-D", "pitch": 1, "persona": "Fast-talking, nervous energy."},
            "Margo Martindale": {"voice": "en-US-Journey-F", "pitch": -2, "persona": "Authoritative, Southern warmth."},
            "JK Simmons": {"voice": "en-US-Journey-D", "pitch": -2, "persona": "Intense, demanding, rhythmic."},
            "CCH Pounder": {"voice": "en-US-Journey-F", "pitch": 1, "persona": "Gravitas, deep resonance."}
        }

def run_improv_session(vpform, output_dir, text_engine, args):
    """
    Main Loop for Improv Mode.
    """
    print(f"üé≠ IMPROV ANIMATOR: {vpform}")
    
    # 1. Config
    defs = FORM_DEFS.get(vpform, FORM_DEFS["24-cartoon"])
    if "alias" in defs: defs = FORM_DEFS[defs["alias"]]
    
    target_duration = args.slength if args.slength > 0 else (defs.get("duration_override", 24 * 60))
    
    session_id = int(time.time())
    session_dir = os.path.join(output_dir, f"session_{session_id}")
    os.makedirs(session_dir, exist_ok=True)
    
    # 2. Seeds & Cast
    if "gahd" in vpform:
        print("[*] G.A.H.D Mode: Fetching 2 Seeds for Movie Pitch...")
        seeds = [get_chaos_seed() for _ in range(2)]
        print(f"    Seeds: {seeds}")
        
        # Restore Improv Logic for GAHD (Ensemble Casting)
        cast = generate_ensemble_cast(text_engine)
        cast_desc = "\n".join([f"- {n}: {d['persona']}" for n, d in cast.items()])
        
        # Dynamic Location
        location = generate_location_context(text_engine, args)
        
        # Override System Prompt for GAHD
        system_prompt = (
            "You are a Writers Room of 4 eccentric screenwriters (Character Actors).\n"
            f"The Cast:\n{cast_desc}\n\n"
            f"The Setting: {location}\n"
            f"The Mission:\n"
            f"Develop a high-concept Movie Pitch that combines these two random elements:\n"
            f"1. {seeds[0]}\n2. {seeds[1]}\n\n"
            "The Rules:\n"
            "1. Generate ONE turn at a time (Speaker + Dialogue + Physical Action).\n"
            "2. Maintain a coherent conversation developing the pitch.\n"
            "3. Style: Fast-paced, witty, collaborative but argumentative.\n"
            "4. Format: JSON {{ 'speaker': 'Name', 'text': 'Dialogue', 'action_prompt': 'Visual description', 'visual_focus': 'Focus' }}\n"
        )
        
        # Dynamic Visual Prompt
        session_visual_style = (
            f"A photorealistic sketch of {location}. Drawn by a preternaturally talented artist. "
            "Hyperrealistic style, clean commercial art. NO WORDS, NO SPEECH BUBBLES. "
            "The artist captures the actors improvising. Dynamic composition."
        )
        

        
    elif "route66" in vpform:
        # Route 66 Logic
        print("[*] Route 66 Mode: Fetching 6 Seeds for The Journey...")
        seeds = [get_chaos_seed() for _ in range(6)]
        print(f"    Seeds: {seeds}")
        
        cast = defs["cast"]
        cast_desc = "\n".join([f"- {n}: {d['persona']}" for n, d in cast.items()])
        system_prompt = defs["system_prompt_template"].format(cast_desc=cast_desc)
        
        # Random location generation logic same as others
        location = generate_location_context(text_engine, args)
        session_visual_style = (
            f"A cinematic shot of {location}. Wide aspect ratio. "
            "Atmospheric lighting, highly detailed 3D environment. "
            "Photorealistic."
        )

    else:
        # Standard Improv
        print("[*] Gathering Chaos Seeds...")
        seeds = [get_chaos_seed() for _ in range(6)]
        print(f"    Seeds: {seeds}")
        
        cast = defs["cast"]
        cast_desc = "\n".join([f"- {n}: {d['persona']}" for n, d in cast.items()])
        system_prompt = defs["system_prompt_template"].format(cast_desc=cast_desc)
        
        # Dynamic Location (Default for standard improv too? Why not)
        location = generate_location_context(text_engine, args)
        session_visual_style = (
            f"A photorealistic sketch of {location}. Drawn by a preternaturally talented artist. "
            "Hyperrealistic style. NO WORDS. Dynamic composition."
        )
    
    # 3. Execution Loop
    total_duration = 0.0
    turn_count = 0
    assets = []
    history = []
    
    # system_prompt is set in block above now
    # system_prompt = defs["system_prompt_template"].format(cast_desc=cast_desc)
    
    while total_duration < target_duration:
        # A. Seed Injection
        seed_idx = int(total_duration // 240)
        current_seed = seeds[seed_idx] if seed_idx < len(seeds) else "The Grand Finale"
        
        # B. Write
        # B. Write
        if "gahd" in vpform:
             prompt = (
                f"Time: {total_duration:.1f}/{target_duration}s.\n"
                f"Current Topic: Pitching '{seeds[0]}' meets '{seeds[1]}'.\n"
                f"History:\n" + "\n".join([f"{h['speaker']}: {h['text']}" for h in history[-10:]]) + 
                "\n\nGenerate next turn (JSON)."
            )
        else:
            seed_idx = int(total_duration // 240)
            current_seed = seeds[seed_idx] if seed_idx < len(seeds) else "The Grand Finale"
            prompt = (
                f"Time: {total_duration:.1f}/{target_duration}s. Seed: {current_seed}\n"
                f"History:\n" + "\n".join([f"{h['speaker']}: {h['text']}" for h in history[-8:]]) + 
                "\n\nGenerate next turn (JSON)."
            )
        
        turn_data = None
        for attempt in range(3):
            try:
                raw = text_engine.generate(system_prompt + "\n\n" + prompt, json_schema=True)
                turn_data = json.loads(raw)
                break
            except Exception as e:
                print(f"    [!] Writer Error: {e}")
                time.sleep(1)
        
        if not turn_data: 
            print("    [!] Skipping turn (Writer failed).")
            continue
            
        speaker = turn_data.get('speaker', 'Unknown')
        text = turn_data.get('text', '...')
        action = turn_data.get('action_prompt', 'Standing.')
        visual = turn_data.get('visual_focus', speaker)
        
        print(f"    [{turn_count}] {speaker}: {text[:40]}...")
        history.append({'speaker': speaker, 'text': text})
        
        # C. Visual (First, so Foley can see it)
        img_path = os.path.join(session_dir, f"turn_{turn_count:04d}.jpg")
        
        # Truncate prompt to avoid CLIP 77 token warning (User Request)
        # Using ~300 chars as safe proxy for 76 tokens
        full_vis_prompt = f"{session_visual_style}\nAction: {action}\nFocus: {visual}"
        if len(full_vis_prompt) > 300:
            visual_prompt = full_vis_prompt[:297] + "..."
        else:
            visual_prompt = full_vis_prompt
            
        # Route 66 Carbonation Logic
        if "route66" in vpform:
            from sassprilla_carbonator import carbonate_prompt
            # Carbonate visual prompt with 3D space awareness
            # Priority: Character > Topic > Scene > Location
            # We map "Character + Action" to the 'Title' slot so Carbonator focuses on it.
            context_seed = current_seed if 'current_seed' in locals() else "The Journey"
            
            # Truncate location if it's too long (User Request: "don't start with 100 chars of location stuff")
            loc_str = location
            if len(loc_str) > 100:
                loc_str = loc_str[:97] + "..."

            carb_prompt = carbonate_prompt(
                title=f"{visual} :: {action}",  # "Song Title" acts as the core subject
                artist=f"Topic: {context_seed}", # "Artist" acts as the thematic lens
                extra_context=f"Location: {loc_str}. Style: {session_visual_style}"
            )
            if carb_prompt:
                # Smart Truncation to avoid FluxBridge naive chop (User Request)
                # Target ~300 chars (approx 75 tokens) for safety
                if len(carb_prompt) > 300:
                    short_prompt = carb_prompt[:300]
                    # Find last sentence-ending punctuation
                    last_punc = max(short_prompt.rfind('.'), short_prompt.rfind('!'), short_prompt.rfind('?'))
                    
                    if last_punc > 150: # Ensure we don't cut too much (keep at least half)
                        carb_prompt = short_prompt[:last_punc+1]
                    else:
                        # Fallback to last space if no punctuation found nearby
                        last_space = short_prompt.rfind(' ')
                        if last_space > 0:
                            carb_prompt = short_prompt[:last_space] + "..."
                        else:
                            carb_prompt = short_prompt # No spaces? Just use hard cut
                
                print(f"       ü´ß Carbonated Visual: {carb_prompt[:50]}... ({len(carb_prompt)} chars)")
                visual_prompt = carb_prompt
            
        if not generate_image(visual_prompt, img_path):
             create_black_frame(img_path)
             
        # D. Audio (Speech)
        wav_path = os.path.join(session_dir, f"turn_{turn_count:04d}.wav")
        voice_info = cast.get(speaker, list(cast.values())[0])
        audio_mode = "kokoro" # User Request: Always use Kokoro
        
        target_voice = voice_info['voice']
        # Always map to Kokoro equivalents since we are forcing audio_mode="kokoro"
        target_voice = map_voice_to_kokoro(target_voice)
            
        final_wav = generate_audio_asset(
            text, wav_path, 
            voice_name=target_voice, 
            pitch=voice_info['pitch'],
            mode=audio_mode
        )
        
        # F. RVC Post-Process
        if args.rvc and final_wav and os.path.exists(final_wav):
            run_rvc_conversion(final_wav, speaker)
        
        if not final_wav:
             print("       [!] Audio failed.")
             continue
             
        duration = get_audio_duration(final_wav) + 0.5 # Pause
        
        # E. Foley (Optional)
        foley_path = None
        if FOLEY_ENABLED:
            foley_prompt = f"{action} in a theater."
            foley_out = os.path.join(session_dir, f"foley_{turn_count:04d}.wav")
            
            # Create a temporay video loop from the image for Foley Visual Context
            temp_vid = os.path.join(session_dir, f"temp_vid_{turn_count:04d}.mp4")
            subprocess.run([
                'ffmpeg', '-loop', '1', '-i', img_path, 
                '-c:v', 'libx264', '-t', str(duration), '-pix_fmt', 'yuv420p', 
                temp_vid, '-y', '-loglevel', 'error'
            ])
            
            generate_foley_asset(foley_prompt, foley_out, video_path=temp_vid, duration=duration)
            
            if os.path.exists(foley_out):
                foley_path = foley_out
                mixed_wav = os.path.join(session_dir, f"mixed_{turn_count:04d}.wav")
                subprocess.run([
                    'ffmpeg', '-i', final_wav, '-i', foley_path,
                    '-filter_complex', 'amix=inputs=2:duration=first:dropout_transition=2',
                    mixed_wav, '-y', '-loglevel', 'error'
                ])
                final_wav = mixed_wav
                
            # Cleanup temp video
            if os.path.exists(temp_vid): os.remove(temp_vid)
        
        assets.append({
            "audio": final_wav, "image": img_path, "duration": duration, 
            "speaker": speaker, "text": text,
            "action_prompt": action, "visual_focus": visual, "foley_prompt": foley_prompt if FOLEY_ENABLED else ""
        })
        
        total_duration += duration
        turn_count += 1
        
    # 4. Stitch / Export
    # Naming Logic: GAHD-{ep}-{ts}
    ep_str = f"{args.ep}-" if args.ep else ""
    final_basename = f"GAHD-{ep_str}{session_id}"
    
    # Target Directory
    gahd_out_dir = os.path.join(output_dir, "gahd-scripts-vids")
    os.makedirs(gahd_out_dir, exist_ok=True)
    
    final_mp4_path = os.path.join(gahd_out_dir, f"{final_basename}.mp4")
    
    stitch_assets(assets, session_dir, final_mp4_path)
    
    # 5. Export XMVP XML
    # Save alongside MP4
    xml_path = os.path.join(gahd_out_dir, f"{final_basename}.xml")
    
    cast_names = list(cast.keys())
    
    # We call helper differently? No, helper creates filename usually?
    # export_xmvp_manifest definition: def export_xmvp_manifest(output_dir, base_name, ...)
    # It constructs path: os.path.join(output_dir, f"{base_name}_manifest.xml")
    # User requested same name for MP4 and XML.
    # helper adds "_manifest.xml". We might want to adjust helper call or helper itself.
    # Let's adjust helper call to use gahd_out_dir and final_basename, but accepting the suffix for now.
    # User said: "same name for the MP4 and XML files" -> GAHD-201-1234.xml vs GAHD-201-1234_manifest.xml?
    # Usually clean names are better.
    # I'll let the helper add _manifest for clarity, or I'll patch helper too if needed.
    # Let's check helper definition below. 
    # Helper: xml_path = os.path.join(output_dir, f"{base_name}_manifest.xml")
    # I'll stick to that convention for now, close enough.
    
    # Determine Template Name based on vpform
    target_template = None
    if "gahd" in vpform: 
        target_template = "GAHD_Template"
    elif "route66" in vpform:
        target_template = "Route66_Template"
    elif "24-" in vpform:
        target_template = "24_Template"

    export_xmvp_manifest(
        gahd_out_dir,
        final_basename,
        assets,
        cast_names,
        seeds=seeds,
        title=f"Improv Session {session_id} (Ep {args.ep})",
        synopsis="A generated improv comedy set.",
        mll_template=target_template
    )
    
    print(f"‚úÖ GAHD Session Complete. Output: {final_mp4_path}")


# --- PODCAST LOGIC (From podcast_animator.py) ---

class Segment:
    def __init__(self, speaker, text):
        self.speaker = speaker
        self.text = text
        self.image_path = None
        self.audio_path = None
        self.duration = 0.0

def parse_transcript_basic(txt_path):
    # Simplified parser
    segments = []
    with open(txt_path, 'r') as f: lines = f.readlines()
    curr_spk = "Unknown"
    curr_txt = []
    
    for line in lines:
        line = line.strip()
        if not line or line.startswith("---"): continue
        match = re.match(r'^([A-Z\s\.]+):', line)
        if match:
            if curr_txt: segments.append(Segment(curr_spk, " ".join(curr_txt)))
            curr_spk = match.group(1).title()
            curr_txt = [line[len(match.group(0)):].strip()]
        else:
            curr_txt.append(line)
    if curr_txt: segments.append(Segment(curr_spk, " ".join(curr_txt)))
    return segments

def run_podcast_processing(triplets_dir, output_dir, args):
    """
    Standard Podcast workflow: Triplet (TXT+JSON+MP3) or Pair (TXT+JSON) -> Video.
    """
    print("[*] Scanning for Podcast Text Pairs...")
    txt_files = glob.glob(os.path.join(triplets_dir, "*.txt"))
    
    for txt_path in txt_files:
        base = os.path.splitext(os.path.basename(txt_path))[0]
        json_path = os.path.join(triplets_dir, f"{base}.json")
        mp3_path = os.path.join(triplets_dir, f"{base}.mp3")
        output_mp4 = os.path.join(output_dir, f"{base}.mp4")
        
        if os.path.exists(output_mp4):
            print(f"    [.] Skipping {base} (Exists)")
            continue
            
        if not os.path.exists(json_path):
            continue
            
        # Determine Mode: Triplet (Sync) or Pair (Gen)
        if os.path.exists(mp3_path):
             print(f"[*] Found TRIPLET: {base}")
             # Sync logic omitted for brevity in this merge, assuming Gen-Only focus for now 
             # or simply calling old logic if needed. 
             # User request emphasized "everything from improv_animator works in content_producer".
             # But let's support Audio Gen (Pair) mode as that's arguably more critical for "Content Producer".
             pass 
        else:
             print(f"[*] Found PAIR: {base} (Generating Audio)")
             process_pair(base, txt_path, json_path, output_mp4, project_id=args.project)

def process_pair(base, txt_path, json_path, output_mp4, project_id=None):
    segments = parse_transcript_basic(txt_path)
    temp_dir = os.path.join(OUTPUT_DIR, f"temp_{base}")
    os.makedirs(temp_dir, exist_ok=True)
    
    processed = []
    
    for i, seg in enumerate(segments):
        print(f"    Processing {i}/{len(segments)}: {seg.speaker}")
        
        # Audio
        wav_path = os.path.join(temp_dir, f"seg_{i}.wav")
        # Simple deterministic voice mapping
        # Hash speaker name to deterministic pitch/voice
        # Using foley_talk defaults handling if we just pass name? 
        # Actually foley_talk needs explicit voice usually, but let's default to Journey-D/F based on name hash
        
        pitch = 0
        voice = "en-US-Journey-D" 
        if output_mp4: # Hack check
            h = hash(seg.speaker)
            voice = "en-US-Journey-D" if h % 2 == 0 else "en-US-Journey-F"
            pitch = (h % 3) - 1 # -1, 0, 1
            
        # FORCE KOKORO
        voice = map_voice_to_kokoro(voice)
        audio_mode = "kokoro"
        
        final_wav = generate_audio_asset(seg.text, wav_path, voice_name=voice, pitch=pitch, mode=audio_mode, project_id=project_id)
        if not final_wav: continue
        
        seg.audio_path = final_wav
        seg.duration = get_audio_duration(final_wav)
        
        # Image
        img_path = os.path.join(temp_dir, f"seg_{i}.png")
        prompt = f"Cinematic shot of {seg.speaker} speaking. Context: '{seg.text[:50]}'"
        if not generate_image(prompt, img_path):
            create_black_frame(img_path)
        seg.image_path = img_path
        
        processed.append(seg)
        time.sleep(1)
        
    # Stitch
    assets = [{"audio": s.audio_path, "image": s.image_path, "duration": s.duration} for s in processed]
    stitch_assets(assets, temp_dir, output_mp4)
    
    # Export XMVP
    cast_names = list(set([s.speaker for s in processed]))
    export_xmvp_manifest(
        os.path.dirname(output_mp4),
        os.path.splitext(os.path.basename(output_mp4))[0],
        processed, # Pass raw segments, helper handles obj
        cast_names,
        title=base,
        synopsis="Podcast Animation"
    )
    
    shutil.rmtree(temp_dir)

def stitch_assets(assets, temp_dir, output_mp4):
    # Quick Stitch
    list_path = os.path.join(temp_dir, "stitch.txt")
    mp4s = []
    
    print(f"   üßµ Stitching {len(assets)} segments...")
    
    for i, a in enumerate(assets):
        # Validate Assets
        img_path = a.get('image')
        aud_path = a.get('audio')
        
        if not img_path or not os.path.exists(img_path):
            print(f"      [!] Missing Image for Seg {i}: {img_path}")
            # Create black frame fallback?
            # For now, just skip to avoid crashing the whole render
            continue
            
        if not aud_path or not os.path.exists(aud_path):
            print(f"      [!] Missing Audio for Seg {i}: {aud_path}")
            continue

        seg_mp4 = os.path.join(temp_dir, f"clip_{i}.mp4")
        
        try:
            subprocess.run([
                'ffmpeg', '-y', '-loop', '1', '-i', img_path, '-i', aud_path,
                '-c:v', 'libx264', '-tune', 'stillimage', '-c:a', 'aac', '-b:a', '192k',
                '-pix_fmt', 'yuv420p', '-shortest', seg_mp4, '-loglevel', 'error'
            ], check=True)
            mp4s.append(seg_mp4)
        except Exception as e:
             print(f"      [!] FFmpeg Failed for Seg {i}: {e}")
             
    if not mp4s:
        print("[-] No valid segments to stitch.")
        return

    with open(list_path, 'w') as f:
        for mp4 in mp4s: f.write(f"file '{mp4}'\n")
        
    try:
        subprocess.run(['ffmpeg', '-f', 'concat', '-safe', '0', '-i', list_path, '-c', 'copy', output_mp4, '-y', '-loglevel', 'error'], check=True)
        print(f"[+] Output Saved: {output_mp4}")
    except Exception as e:
        print(f"[-] Final Stitch Failed: {e}")

def export_xmvp_manifest(output_dir, base_name, assets, cast_names, seeds=None, title=None, synopsis="Generated Content", mll_template=None):
    """
    Generates and saves the XMVP XML manifest.
    """
    try:
        # 1. Models
        vp = VPForm(
            name="content-producer-v1",
            fps=24,
            description="Content Producer Output",
            mime_type="video/mp4"
        )
        
        cssv = CSSV(
            constraints=Constraints(width=1024, height=1024, fps=24),
            scenario=f"Content based on {base_name}",
            situation=synopsis,
            vision="Cinematic/Generated",
            mll_template=mll_template
        )
        
        story = Story(
            title=title if title else base_name,
            synopsis=synopsis,
            characters=cast_names,
            theme="Improv/Podcast",
            mll_template=mll_template
        )
        
        portions = []
        for i, item in enumerate(assets):
            # Handle both Dict (Improv) and Segment Obj (Podcast)
            if isinstance(item, dict):
                spk = item['speaker']
                txt = item.get('text', '')
                dur = item['duration']
            else:
                spk = item.speaker
                txt = item.text
                dur = item.duration
                
            portions.append(Portion(
                id=i+1,
                duration_sec=dur,
                content=f"{spk}: {txt}"
            ))
            
        xmvp_data = {
            "VPForm": vp,
            "CSSV": cssv,
            "Story": story,
            "Portions": [p.model_dump() for p in portions]
        }
        
        if seeds:
            xmvp_data["ChaosSeeds"] = seeds
        
        # 2. Detailed Dialogue Script (Line-by-Line)
        dialogue_lines = []
        for item in assets:
            if isinstance(item, dict):
                 # Improv Dict
                 dl = mvp_shared.DialogueLine(
                     character=item['speaker'],
                     text=item.get('text', ''),
                     duration=item['duration'],
                     action=item.get('action_prompt', ''), # Stored earlier?
                     foley=item.get('foley_prompt', ''),   # Wait, need to store these in assets in main loop
                     visual_focus=item.get('visual_focus', '')
                 )
            else:
                 # Podcast Segment
                 dl = mvp_shared.DialogueLine(
                     character=item.speaker,
                     text=item.text,
                     duration=item.duration
                 )
            dialogue_lines.append(dl)
            
        xmvp_data["Dialogue"] = mvp_shared.DialogueScript(lines=dialogue_lines)
            
        xml_path = os.path.join(output_dir, f"{base_name}_manifest.xml")
        save_xmvp(xmvp_data, xml_path)
        print(f"    üìú XML Manifest Saved: {xml_path}")
        
    except Exception as e:
        print(f"[-] XMVP Export Failed: {e}") 
        import traceback
        traceback.print_exc() 


# --- MAIN ---

def run_thax_douglas_session(band_name, poem, output_dir):
    """
    Generates a Thax Douglas poem visualization.
    Voice: am_michael (Kokoro)
    Visuals: Band acting out poem.
    """
    if not band_name or not poem:
        print("[-] Error: --band and --poem required for thax-douglas mode.")
        return

    session_id = int(time.time())
    session_dir = os.path.join(output_dir, f"session_{session_id}")
    os.makedirs(session_dir, exist_ok=True)
    
    print(f"üé∏ Starting Thax Douglas Session: {band_name}...")
    
    # 1. Parse Poem
    # User Request: Replace slashes with periods to fix TTS flow
    # "The road is long / And my feet are tired" -> "The road is long. And my feet are tired"
    poem_sanitized = poem.replace("/", ".")
    
    # Split by newlines first
    stanzas = [line.strip() for line in poem_sanitized.split('\n') if line.strip()]
    
    # 1b. Smart Splitting (If single block detected)
    # If explicit newlines weren't used, we split by sentences to create more visual variety.
    if len(stanzas) == 1 and len(stanzas[0]) > 50:
         text_block = stanzas[0]
         # Split by punctuation followed by space (. ! ?)
         # We use a positive lookbehind to keep the punctuation attached to the sentence.
         # Regex: (?<=[.!?])\s+
         split_stanzas = re.split(r'(?<=[.!?])\s+', text_block)
         stanzas = [s.strip() for s in split_stanzas if s.strip()]
         print(f"   ‚úÇÔ∏è Auto-Split Poem into {len(stanzas)} lines based on punctuation.")
         
    if not stanzas:
        stanzas = [poem] # Just one big block
        
    # 1.5 Prepend Band Name (Thax Introduction)
    # Thax says the band name first.
    stanzas.insert(0, f"{band_name}.")
    
    assets = []
    
    # 2. Loop
    for i, line in enumerate(stanzas):
        print(f"   üìú Stanza {i+1}/{len(stanzas)}: {line[:40]}...")
        
        # Audio (Thax Voice)
        # Force Kokoro 'am_michael' (or am_adam) for Thax
        # If Foley disabled or Cloud Mode, fall back to Journey-D via foley_talk defaults
        audio_name = f"thax_line_{i:03d}.wav"
        audio_path = os.path.join(session_dir, audio_name)
        
        # We use 'am_michael' as the 'voice_name'. foley_talk handles mapping/generation.
        # If mode='cloud', it maps to Journey-D naturally? No, foley_talk doesn't map am_michael to journey unless we map it.
        # So we pass 'am_michael' and mode='kokoro' if local, else 'en-US-Journey-D'.
        
        voice_target = "am_michael" if LOCAL_MODE else "en-US-Journey-D"
        mode_target = "kokoro" if LOCAL_MODE else "cloud"
        
        # Use Thax Engine (RVC) if possible, else standard
        thax_engine = get_thax_engine()
        success = thax_engine.generate(line, audio_path)
        final_audio = audio_path if success else None
        
        if not final_audio:
            print(f"   [-] Audio Gen Failed for line {i}")
            # Use silence as fallback?
            continue
            
        duration = get_audio_duration(final_audio)
        
        # Visual
        img_name = f"thax_vis_{i:03d}.jpg"
        img_path = os.path.join(session_dir, img_name)
        
        # Simplified Visual Prompt (User Request: "Exact line only")
        # Removing "Concert/Band" context.
        # Adding minimal style suffix for Flux quality.
        prompt = f"{line}, cinematic, photorealistic, 4k."
        
        # Truncate for Flux/CLIP safety
        if len(prompt) > 300: prompt = prompt[:297] + "..."
        
        if generate_image(prompt, img_path):
            assets.append({
                "path": img_path,
                "duration": duration, # Image holds for audio duration
                "audio": final_audio  # Attach audio to this slide
            })
        else:
            print("   [-] Image Gen Failed.")
            
    # 3. Stitch
    print("   üßµ stitching slides...")
    video_clips = []
    for i, asset in enumerate(assets):
        slide_mp4 = os.path.join(session_dir, f"slide_{i:03d}.mp4")
        # FFmpeg Image + Audio -> MP4
        # Loop image for duration of audio
        cmd = [
            "ffmpeg", "-y",
            "-loop", "1", "-i", asset['path'],
            "-i", asset['audio'],
            "-c:v", "libx264", "-tune", "stillimage", "-c:a", "aac", "-b:a", "192k",
            "-shortest", 
            "-pix_fmt", "yuv420p",
            slide_mp4
        ]
        try:
            subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            video_clips.append(slide_mp4)
        except Exception as e:
            print(f"   [-] Slide Mux Failed: {e}")
            
    # Concat clips
    if video_clips:
        thax_out_dir = os.path.join(output_dir, "thax-douglas")
        os.makedirs(thax_out_dir, exist_ok=True)
        final_filename = f"Thax-{band_name.replace(' ', '')}-{session_id}.mp4"
        final_path = os.path.join(thax_out_dir, final_filename)
        
        files_txt = os.path.join(session_dir, "clips.txt")
        with open(files_txt, 'w') as f:
            for v in video_clips:
                f.write(f"file '{v}'\n")
                
        cmd_concat = [
            "ffmpeg", "-y", "-f", "concat", "-safe", "0",
            "-i", files_txt,
            "-c", "copy",
            final_path
        ]
        subprocess.run(cmd_concat, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        print(f"‚úÖ Thax Douglas Session Complete. Output: {final_path}")
        
    else:
        print("‚ùå No slides generated.")

def generate_image(prompt, output_path):
    """
    Wrapper for Image Gen (Local Flux vs Cloud Gemini).
    """
    global FLUX_BRIDGE
    if LOCAL_MODE:
        if not FLUX_BRIDGE:
            # Init Bridge
            from flux_bridge import get_flux_bridge
            FLUX_BRIDGE = get_flux_bridge(FLUX_MODEL_PATH)
        
        # Flux Gen
        # Use Global Args for dims (defaults 1024x576)
        # We need to access args from main scope or pass them. 
        # Easier to check definitions or assume global if simple script, 
        # but cleaner to user explicit args. 
        # Since this is a simple wrapper, let's rely on the module-level ARGS if we store them, 
        # OR better: update signature to accept width/height and update calls.
        # Given the "All vpforms" request, let's use a module-level default or quick look up.
        
        # For this refactor, let's just use the defaults we just added to CLI, 
        # but we need to access them. 
        # We'll use a globally injected 'GENERATION_DIMS' tuple set in main.
        w, h = GENERATION_DIMS if 'GENERATION_DIMS' in globals() else (1024, 576)
        
        print(f"   üé® [Flux] {prompt[:40]}... ({w}x{h})")
        img = FLUX_BRIDGE.generate(prompt, width=w, height=h, steps=4)
        if img:
            try:
                # Ensure dir exists (Defensive Check)
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                img.save(output_path)
                return True
            except Exception as e:
                print(f"   [!] Image Save Failed: {e}")
                return False
        return False
    else:
        return False 

# --- FULLMOVIE STILL MODE ---

def get_segment_for_time(manifest: Manifest, time_sec: float, fps: float = 24.0):
    """Finds the visual segment active at the given time."""
    frame = int(time_sec * fps)
    # Simple linear search (manifests are small enough)
    for seg in manifest.segs:
        if seg.start_frame <= frame < seg.end_frame:
            return seg
    
    # Fallback: Closest or Last
    if manifest.segs:
        return manifest.segs[-1]
    return None

def run_fullmovie_still_mode(xml_path, output_dir, text_engine, args):
    """
    Ingests an existing XMVP XML and generates a slideshow movie.
    One frame per dialogue line.
    """
    print(f"üé¨ FULLMOVIE-STILL ANIMATOR: {xml_path}")
    
    # 1. Load XMVP
    try:
        if xml_path.endswith('.xml'):
            print("    [üì¶] Loading XMVP XML Manifest...")
            raw_json = load_xmvp(xml_path, "Manifest")
            
            if not raw_json:
                print("    [!] No <Manifest> key. Checking for <Portions> (Script Mode)...")
                raw_portions = load_xmvp(xml_path, "Portions")
                if raw_portions:
                    # JIT Migration: Convert Portions -> Manifest
                    print("    [Build] Converting Portions to Manifest...")
                    portions = [Portion.model_validate(p) for p in json.loads(raw_portions)]
                    
                    # Flatten Dialogue
                    all_lines = []
                    # Create Segments from Portions as fallback
                    segs = []
                    
                    current_frame = 0
                    fps = 24
                    
                    for p in portions:
                       dur_frames = int(p.duration_sec * fps)
                       segs.append(Seg(
                           id=p.id, 
                           start_frame=current_frame, 
                           end_frame=current_frame + dur_frames,
                           prompt=p.content # Use portion content as visual prompt
                       ))
                       current_frame += dur_frames
                       
                       if p.dialogue:
                           for line in p.dialogue:
                               all_lines.append(line)
                       else:
                           # Fallback: Parse "Character: Text" from content
                           # Expected format: "Name: spoken text..."
                           if ":" in p.content:
                               parts = p.content.split(":", 1)
                               char_name = parts[0].strip()
                               spoken_text = parts[1].strip()
                               
                               # Create on-the-fly line
                               fallback_line = DialogueLine(
                                   character=char_name,
                                   text=spoken_text,
                                   action="speaking", # Default action
                                   visual_focus=char_name
                               )
                               all_lines.append(fallback_line)
                               
                    manifest = Manifest(
                        segs=segs,
                        dialogue=DialogueScript(lines=all_lines)
                    )
                else:
                    print("[-] No <Manifest> or <Portions> found in XML.")
                    return
            else:
                manifest = Manifest.model_validate_json(raw_json)
        else:
            manifest = load_manifest(xml_path)
    except Exception as e:
        print(f"[-] Failed to load Manifest: {e}")
        return

    if not manifest.dialogue or not manifest.dialogue.lines:
        print("[-] XML has no dialogue lines to animate.")
        return
        
    # 2. Config & Cast
    session_id = int(time.time())
    session_dir = os.path.join(output_dir, f"fms_session_{session_id}")
    os.makedirs(session_dir, exist_ok=True)
    
    # Extract Cast
    all_chars = set(line.character for line in manifest.dialogue.lines)
    print(f"    [üë•] Cast Found: {all_chars}")

    # --- MOVIE-LEVEL LORA (MLL) LOGIC ---
    # --- MOVIE-LEVEL LORA (MLL) LOGIC ---
    movie_title = "Unknown_Movie"
    target_name = None
    
    try:
        # Load Story
        raw_story = load_xmvp(xml_path, "Story")
        if raw_story:
            story_obj = Story.model_validate_json(raw_story)
            movie_title = story_obj.title.replace(" ", "_")
            if story_obj.mll_template:
                 target_name = story_obj.mll_template
        
        # Load Bible (Preferred Source for Template)
        raw_bib = load_xmvp(xml_path, "Bible") 
        if raw_bib:
             # Just parse as dict to avoid strict validation if needed, or use CSSV
             bib_data = json.loads(raw_bib)
             if "mll_template" in bib_data and bib_data["mll_template"]:
                  target_name = bib_data["mll_template"]
                  
    except Exception as e:
        print(f"    [!] Error reading XMVP Metadata: {e}")
        pass
        
    final_lora_name = target_name if target_name else movie_title
    if target_name:
         print(f"    üè∑Ô∏è  MLL Template Detected: {target_name}")
        
    print(f"    [üé•] Movie Title: {movie_title} (LoRA Target: {final_lora_name})")
    
    # Check LoRA
    lora_path = os.path.join("adapters/movies", f"{final_lora_name}.safetensors")
    
    # Logic:
    # 1. If LoRA missing -> Must Train.
    # 2. If Template Active -> Must Prep (Additive) + Train (Update).
    # 3. Else (Standard) -> Skip if exists.
    
    should_train = False
    if not os.path.exists(lora_path):
        should_train = True
    elif target_name: 
        # Additive Mode for Templates
        print(f"    [+] MLL Template Active ({target_name}). Enforcing Additive Prep & Training.")
        should_train = True
        
    if should_train and args.local:
        print(f"    [‚ö†Ô∏è] LoRA Missing: {lora_path}")
        print("    [üî®] Starting Pre-Production: Generating Character Sheets & Training MLL...")
        
        try:
            # 1. Prep Assets
            # prep_movie_assets reads mll_template from Bible automatically.
            cmd_prep = [sys.executable, "prep_movie_assets.py", "--xml", xml_path, "--force"]
            print("       > Running prep_movie_assets.py...")
            subprocess.run(cmd_prep, check=True)
            
            # 2. Train LoRA
            # Dataset will be in target_name folder if prep_movie_assets found template
            dataset_dir = os.path.join("z_training_data/movies", final_lora_name, "dataset")
            cmd_train = [sys.executable, "train_mll.py", "--dataset", dataset_dir, "--name", final_lora_name, "--steps", "200"]
            print("       > Running train_mll.py...")
            subprocess.run(cmd_train, check=True)
            
        except subprocess.CalledProcessError as e:
            print(f"    [!] Pre-Production Failed: {e}. Proceeding without LoRA.")
            
    if os.path.exists(lora_path) and args.local:
         print(f"    [üíâ] Loading Movie-Level LoRA: {lora_path}")
         # Force Init Bridge
         from flux_bridge import get_flux_bridge
         global FLUX_BRIDGE
         FLUX_BRIDGE = get_flux_bridge(FLUX_MODEL_PATH)
         FLUX_BRIDGE.load_lora(lora_path)
    
    # Assign Voices (Deterministic Kokoro)
    # Use foley_talk helper logic but we need to fetch voices locally?
    # foley_talk.assign_kokoro_voice_deterministic takes (name, available_voices).
    # We can pass empty list to get defaults/fallback, or try to load bridge to get list.
    # For speed, let's just assume defaults or hardcoded safe list for now to ensure consistency without heavy init.
    # Actually, we rely on generate_audio_asset(mode="kokoro") eventually.
    # But we want to decide the mapping UP FRONT.
    
    safe_voices = ["af_bella", "af_sarah", "am_michael", "am_adam", "af_nicole", "bm_george"]
    cast_map = {}
    for char in all_chars:
        voice, pitch = assign_kokoro_voice_deterministic(char, safe_voices)
        cast_map[char] = {"voice": voice, "pitch": pitch}
        print(f"       + {char} -> {voice} (p{pitch})")
        
    # 3. Generation Loop
    assets = []
    
    # Get FPS from args or XML? Content Producer args has no FPS usually?
    # definitions sets default 0.5. But Segments in XML are based on 24 usually (Full Movie).
    # We should respect the XML's context if possible (in Bible). 
    # But `load_manifest` only loads Manifest part. 
    # Let's assume 24 for time calculations if standard.
    fps = 24.0 
    
    total_lines = len(manifest.dialogue.lines)
    
    for i, line in enumerate(manifest.dialogue.lines):
        print(f"    [{i+1}/{total_lines}] {line.character}: {line.text[:40]}...")
        
        # A. Visual
        # Find context
        seg = get_segment_for_time(manifest, line.start_offset, fps)
        
        # Construct Prompt
        # Combine: Seg Prompt + Action + Focus
        base_prompt = seg.prompt if seg else "A cinematic scene."
        visual_prompt = f"{base_prompt} Action: {line.action}. Focus: {line.visual_focus}. Character: {line.character}."
        
        # Output Path
        img_path = os.path.join(session_dir, f"frame_{i:04d}.jpg")
        
        # Generate (Flux/Gemini)
        # Use our helper
        if not generate_image(visual_prompt, img_path):
             create_black_frame(img_path)
             
        # B. Audio (Kokoro)
        wav_path = os.path.join(session_dir, f"line_{i:04d}.wav")
        voice_info = cast_map.get(line.character, {"voice": "af_bella", "pitch": 0})
        
        final_wav = generate_audio_asset(
            line.text, wav_path, 
            voice_name=voice_info['voice'], 
            pitch=voice_info['pitch'],
            mode="kokoro"
        )
        
        # Handle Audio Failure
        if not final_wav:
             print(f"       [-] Audio Gen Failed for Line {i}. Creating Silence.")
             create_silent_wav(wav_path, duration=2.0)
             final_wav = wav_path
             duration = 2.0
        else:
             duration = get_audio_duration(final_wav) + 0.1 # Minimal padding on speech
             
        assets.append({
            "audio": final_wav, "image": img_path, "duration": duration,
            "speaker": line.character, "text": line.text
        })
        
        # --- PAUSE / PACING SEGMENT ---
        # Insert a silence segment between lines.
        # Sometimes hold the last image, sometimes cut to a new one (Reaction/Atmosphere).
        
        pause_dur = random.uniform(1.0, 2.0) # Organic variance
        pause_wav = os.path.join(session_dir, f"pause_{i:04d}.wav")
        create_silent_wav(pause_wav, duration=pause_dur)
        
        # Decide Visual: Hold or Cut?
        # 30% chance of a new "Reaction/Silent" shot.
        if random.random() < 0.3:
            pause_type = "visual"
            # Modify prompt for silence
            # "A cinematic scene... Action: [Action]. Focus: [Focus]... (Silent moment of reflection)"
            pause_prompt = f"{visual_prompt} (Character is silent, listening, or reacting. Atmospheric moment.)"
            pause_img = os.path.join(session_dir, f"frame_{i:04d}_pause.jpg")
            
            if not generate_image(pause_prompt, pause_img):
                # Fallback to holding previous image if gen fails
                pause_img = img_path
        else:
            pause_type = "hold"
            pause_img = img_path

        assets.append({
            "audio": pause_wav, "image": pause_img, "duration": pause_dur,
            "speaker": None, "text": "[Silence]"
        })
        
    # 4. Stitch
    final_basename = f"FMS-{session_id}"
    fms_out_dir = os.path.join(output_dir, "fullmovie-stills")
    os.makedirs(fms_out_dir, exist_ok=True)
    
    final_mp4_path = os.path.join(fms_out_dir, f"{final_basename}.mp4")
    stitch_assets(assets, session_dir, final_mp4_path) # stitch_assets is defined in podcast section, need to check scope or move it.
    # checking file... stitch_assets is defined inside run_podcast_processing or global?
    # It was NOT shown in the view_file of content_producer.py.
    # Wait, I viewed lines 1-800 and 1150+.
    # I probably missed it or it's named differently.
    # In run_improv_session (line 757) it calls `stitch_assets`. 
    # I must assume it is available in global scope.
    
    # 5. Metadata
    print(f"‚úÖ FullMovie-Still Complete. Output: {final_mp4_path}")

def create_silent_wav(path, duration=1.0):
    subprocess.run(["ffmpeg", "-y", "-f", "lavfi", "-i", f"anullsrc=r=24000:cl=mono", "-t", str(duration), path], 
                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

def pad_audio_file(path, pad_duration):
    """Appends silence to the end of a wav file."""
    if not os.path.exists(path): return
    
    tmp_path = path.replace(".wav", "_padded.wav")
    try:
        # pad_dur implies adding duration. 
        # filter: apad=pad_dur=X adds X seconds.
        subprocess.run([
            "ffmpeg", "-y", "-i", path, "-af", f"apad=pad_dur={pad_duration}", 
            "-c:a", "pcm_s16le", tmp_path, "-loglevel", "error"
        ], check=True)
        
        os.replace(tmp_path, path)
    except Exception as e:
        print(f"[-] Failed to pad audio {path}: {e}")
        if os.path.exists(tmp_path): os.remove(tmp_path)

def main():
    global LOCAL_MODE, FOLEY_ENABLED

    parser = argparse.ArgumentParser(description="Content Producer v0.5 (Unified)")
    
    # Global/Shared Positional Args
    import definitions
    definitions.add_global_vpform_args(parser)
    
    parser.add_argument("--vpform", default=None, help="Vision Platonic Form (e.g. 24-podcast, gahd-podcast)")
    parser.add_argument("--project", help="Project override (stub)")
    parser.add_argument("--ep", type=int, help="Episode number (stub)")
    parser.add_argument("--local", action="store_true", help="Use Local Engines (Flux + Kokoro)")
    parser.add_argument("--foley", choices=["on", "off"], default="off", help="Enable Generative Foley")
    parser.add_argument("--slength", type=float, default=0.0, help="Override duration in seconds")
    parser.add_argument("--fc", action="store_true", help="Code Painter Mode (Experimental)")
    parser.add_argument("--geminiapi", action="store_true", help="Force Cloud Gemini API for Text (Disable Local Gemma default)")
    parser.add_argument("--band", type=str, help="Band Name (for thax-douglas mode)")
    parser.add_argument("--poem", type=str, help="Poem Text (for thax-douglas mode)")
    parser.add_argument("--w", type=int, default=1024, help="Width (Default: 1024)")
    parser.add_argument("--h", type=int, default=576, help="Height (Default: 576)")
    parser.add_argument("--location", type=str, help="Override visual location (e.g. 'a haunted hotel')")
    parser.add_argument("--rvc", action="store_true", help="Enable RVC (Retrieval Voice Conversion) for Cast")
    parser.add_argument("--xml", type=str, help="Input XMVP XML path (for fullmovie-still mode)")
    parser.add_argument("--xb", type=str, help="Alias for --xml (XMVP Bible/Manifest)")
    
    args, unknown = parser.parse_known_args()
    
    # Handle aliases via registry
    args.vpform = definitions.parse_global_vpform(args, current_default="24-podcast")
    
    print(f"DEBUG: Args Parsed. VPForm: {args.vpform}")


    
    LOCAL_MODE = args.local
    FOLEY_ENABLED = (args.foley == "on")
    
    # Set Global Generation Dims
    global GENERATION_DIMS
    GENERATION_DIMS = (args.w, args.h)

    # --- TEXT ENGINE LOGIC ---
    # Default to "local_gemma" for text to save quota, unless user forces --geminiapi
    if not args.geminiapi:
        print("   [Engine] Defaulting Text Engine to Local Gemma (Saving API Quota)...")
        os.environ["TEXT_ENGINE"] = "local_gemma"
        # We assume local_gemma path is set in env_vars or defaults to mlx-community/gemma-2-9b-it-4bit
    else:
        print("   [Engine] Text Engine Forced to Gemini API (Cloud).")
    
    print(f"üé¨ CONTENT PRODUCER | Local: {LOCAL_MODE} | Foley: {FOLEY_ENABLED} | Form: {args.vpform}")
    
    # 0. Thax Douglas Mode
    if args.vpform == "thax-douglas":
        run_thax_douglas_session(args.band, args.poem, OUTPUT_DIR)
        return

    # 0.5 FullMovie Still Mode
    if args.vpform == "fullmovie-still":
        xml_input = args.xml or args.xb
        if not xml_input:
            print("[-] --xml (or --xb) argument required for fullmovie-still mode.")
            return
        run_fullmovie_still_mode(xml_input, OUTPUT_DIR, None, args)
        return

    if args.vpform and ("podcast" in args.vpform or "cartoon" in args.vpform):
        # 1. Generative Modes (GAHD, 24-podcast, 10-podcast, Route 66)
        if "gahd" in args.vpform or "24" in args.vpform or "10" in args.vpform or "route66" in args.vpform:
            text_engine = TextEngine()
            run_improv_session(args.vpform, OUTPUT_DIR, text_engine, args)
        
        else:
             print(f"[-] Unknown/Legacy VPForm '{args.vpform}'. Generative mode only.")
             # Removed run_podcast_processing call
    else:
        print("[-] No valid VPForm specified. Use --vpform [route66-podcast|gahd-podcast|24-podcast|10-podcast]")

if __name__ == "__main__":
    main()
