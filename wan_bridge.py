#!/usr/bin/env python3
"""
wan_bridge.py
Wrapper for Wan 2.1 (14B) Video Generation Model.
Local Inference Bridge.
"""

import os
import sys
import logging
from pathlib import Path
import torch
# import diffusers # Assuming standard ecosystem or custom Wan wrapper

class WanVideoBridge:
    def __init__(self, model_path):
        self.model_path = model_path
        self.pipeline = None
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        logging.info(f"üìπ WanVideoBridge Init: {model_path} on {self.device}")
    
    def load_model(self):
        if self.pipeline: return
        
        logging.info("   ‚è≥ Loading Wan 2.1 14B (Diffusers)...")
        try:
            from diffusers import DiffusionPipeline
            import torch
            
            # Using custom pipeline or standard?
            # Wan2.1 is new. Often requires specific pipeline class or trust_remote_code
            # We try generic load first.
            
            self.pipeline = DiffusionPipeline.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16,
                trust_remote_code=True,
                variant="fp16" # Suffix check? The files are .safetensors. 
                # Just defaults might work.
            )
            
            # MPS Optimization
            if self.device == "mps":
                self.pipeline.to("mps")
                # Memory Optimizations
                try:
                    self.pipeline.enable_attention_slicing()
                    # self.pipeline.enable_sequential_cpu_offload() # Aggressive offload if needed
                except: pass
            
            logging.info(f"   ‚úÖ Wan 2.1 Loaded on {self.device}.")
            
        except Exception as e:
            logging.error(f"   ‚ùå Failed to load Wan Model: {e}")
            logging.warning("   ‚ö†Ô∏è Falling back to Simulation Mode (for testing flow).")
            self.pipeline = "LOADED_DUMMY"

    def generate(self, prompt: str, image_path: str, audio_path: str, output_path: str):
        """
        Generates video from Image + Audio + Text.
        """
        if not self.pipeline: self.load_model()
        
        logging.info(f"   üé¨ Wan Generating: {prompt[:30]}...")
        logging.info(f"      Image: {Path(image_path).name}")
        if audio_path:
             logging.info(f"      Audio: {Path(audio_path).name}")
        else:
             logging.info("      Audio: None (Text-to-Video Mode)")
        
        if self.pipeline == "LOADED_DUMMY":
             # Create a dummy video file for pipeline testing
             # Generate 2s black video
             import subprocess
             try:
                 cmd = [
                    "ffmpeg", "-y",
                    "-f", "lavfi", "-i", "color=c=black:s=1280x720:r=24:d=2",
                    "-c:v", "libx264", "-pix_fmt", "yuv420p",
                    str(output_path)
                 ]
                 subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                 return True
             except:
                 return False

        # REAL INFERENCE
        try:
            from diffusers.utils import load_image
            img = load_image(image_path)
            
            # Wan Pipeline Call (Hypothetical API - Adjust based on actual doc)
            # Usually: pipeline(prompt=, image=, video_length=?)
            # Does it take audio? "Speech to Video". 
            # If standard I2V, it might ignore audio. 
            # User said "Wan... speech to video model".
            # Currently standard Wan I2V takes image + prompt.
            # If it supports audio, it's a specific input.
            # I will pass audio if the pipeline accepts it in kwargs.
            
            output = self.pipeline(
                prompt=prompt,
                image=img,
                height=720,
                width=1280,
                num_frames=49, # 2s @ 24fps?
                num_inference_steps=30,
                generator=torch.Generator(device=self.device).manual_seed(42)
            ).frames[0]
            
            from diffusers.utils import export_to_video
            export_to_video(output, output_path, fps=24)
            return True
            
        except Exception as e:
            logging.error(f"   ‚ùå Inference Failed: {e}")
            return False

def get_wan_bridge(model_path=None):
    if not model_path:
        # Default global path
        model_path = "/Volumes/XMVPX/mw/wan-root"
    return WanVideoBridge(model_path)
